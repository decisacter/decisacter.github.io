(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{151:function(e,s,i){"use strict";i.r(s);var a=function(){var e=this.$createElement;this._self._c;return this._m(0)};a._withStripped=!0;var n=i(0),t=Object(n.a)({},a,[function(){var e=this,s=e.$createElement,i=e._self._c||s;return i("section",[i("h2",[e._v("Descending into ML")]),e._v(" "),i("section",[i("h3",[e._v("Training and Loss")]),e._v(" "),i("p",[i("strong",[e._v("Training")]),e._v(" a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called "),i("strong",[e._v("empirical risk minimization")]),e._v(".")]),e._v(" "),i("p",[i("strong",[e._v("Loss")]),e._v(" is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero. The goal of training a model is to find a set of weights and biases that have "),i("em",[e._v("low")]),e._v(" loss, on average, across all examples.")]),e._v(" "),i("section",[i("h4",[e._v("Squared loss: a popular loss function")]),e._v(" "),i("p",[e._v("The linear regression models we'll examine here use a loss function called "),i("strong",[e._v("squared loss")]),e._v(" (also known as "),i("strong",[e._v("L"),i("sub",[e._v("2")]),e._v(" loss")]),e._v("). The squared loss for a single example is as follows:")]),e._v(" "),i("pre",[e._v("  = the square of the difference between the label and the prediction = (observation - prediction("),i("b",[e._v("x")]),e._v("))"),i("sup",[e._v("2")]),e._v(" = (y - y')"),i("sup",[e._v("2")]),e._v(" ")]),e._v(" "),i("p",[i("strong",[e._v("Mean square error")]),e._v(" ("),i("strong",[e._v("MSE")]),e._v(") is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples:")]),e._v(" "),i("div",[e._v(" $$ MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 $$ ")]),e._v(" "),i("p",[e._v("where:")]),e._v(" "),i("ul",[i("li",[e._v("\\((x, y)\\) is an example in which \n          "),i("ul",[i("li",[e._v("\\(x\\) is the set of features (for example, chirps/minute, age, gender) that the model uses to make predictions.")]),e._v(" "),i("li",[e._v("\\(y\\) is the example's label (for example, temperature).")])])]),e._v(" "),i("li",[e._v("\\(prediction(x)\\) is a function of the weights and bias in combination with the set of features \\(x\\).")]),e._v(" "),i("li",[e._v("\\(D\\) is a data set containing many labeled examples, which are \\((x, y)\\) pairs.")]),e._v(" "),i("li",[e._v("\\(N\\) is the number of examples in \\(D\\).")])])])])])}],!1,null,null,null);t.options.__file="docs/ml/mlcc/Descending.vue";s.default=t.exports}}]);